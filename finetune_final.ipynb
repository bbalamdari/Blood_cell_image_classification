{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uaoce2gy5o-4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DLNUhCws52nA"
      },
      "outputs": [],
      "source": [
        "project_dir = \"/content/drive/My Drive/OMSCS/CS7643/project/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hJxlakm7xy3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.listdir(project_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhE7IRcUBojd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main logic"
      ],
      "metadata": {
        "id": "IoXPED4c9Ozc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r4i-fVpn9obo"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import copy\n",
        "import json\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "NUM_CLASSES = 4\n",
        "\n",
        "# ImageNet normalization values\n",
        "IMAGENET_NORMALIZE = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "\n",
        "# project_dir = './'\n",
        "\n",
        "def get_model(model_name, dilation=False):\n",
        "    if model_name == 'resnet50':\n",
        "        model = models.resnet50(pretrained=True)\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        if dilation:\n",
        "            for name, module in model.layer4.named_modules():\n",
        "                if isinstance(module, nn.Conv2d):\n",
        "                    if 'conv2' in name:\n",
        "                        module.dilation = (2, 2)\n",
        "                        module.padding = (2, 2)\n",
        "                    for param in module.parameters():\n",
        "                        param.requires_grad = True\n",
        "\n",
        "        num_features = model.fc.in_features\n",
        "        model.fc = nn.Linear(num_features, NUM_CLASSES)\n",
        "        input_size = 224\n",
        "        resize_size = 256\n",
        "    elif model_name == 'densenet121':\n",
        "        model = models.densenet121(pretrained=True)\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        if dilation:\n",
        "            for name, module in model.features.denseblock4.named_modules():\n",
        "                if isinstance(module, nn.Conv2d):\n",
        "                    if 'conv2' in name:\n",
        "                        module.dilation = (2, 2)\n",
        "                        module.padding = (2, 2)\n",
        "                    for param in module.parameters():\n",
        "                        param.requires_grad = True\n",
        "\n",
        "        num_features = model.classifier.in_features\n",
        "        model.classifier = nn.Linear(num_features, NUM_CLASSES)\n",
        "        input_size = 224\n",
        "        resize_size = 256\n",
        "    elif model_name == 'mobilenet_v2':\n",
        "        model = models.mobilenet_v2(pretrained=True)\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        if dilation:\n",
        "            for module in model.features[-2].modules():\n",
        "                for param in module.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "            for module in model.features[-1].modules():\n",
        "                for param in module.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "        model.features[17].conv[1][0].padding = (2, 2)\n",
        "        model.features[17].conv[1][0].dilation = (2, 2)\n",
        "\n",
        "        num_features = model.classifier[-1].in_features\n",
        "        model.classifier[-1] = nn.Linear(num_features, NUM_CLASSES)\n",
        "\n",
        "        input_size = 224\n",
        "        resize_size = 256\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model name. Choose from 'resnet50', 'densenet121', 'efficientnet_b5'\")\n",
        "    return model, input_size, resize_size\n",
        "\n",
        "def train(parameters, suffix=\"0\"):\n",
        "    print(parameters)\n",
        "\n",
        "    # use gpu if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model, input_size, resize_size = get_model(parameters[\"model\"], parameters[\"dilation\"])\n",
        "    model = model.to(device)\n",
        "\n",
        "    print(model)\n",
        "\n",
        "    # Training data\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        IMAGENET_NORMALIZE,\n",
        "    ])\n",
        "    train_dataset = datasets.ImageFolder(root=parameters[\"train_dir\"], transform=train_transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=parameters[\"batch_size\"], shuffle=True, num_workers=parameters[\"num_workers\"])\n",
        "\n",
        "    # Test data\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize(resize_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        IMAGENET_NORMALIZE,\n",
        "    ])\n",
        "    test_dataset = datasets.ImageFolder(root=parameters[\"test_dir\"], transform=test_transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=parameters[\"batch_size\"], shuffle=False, num_workers=parameters[\"num_workers\"])\n",
        "\n",
        "    # Loss fn\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr=parameters[\"learning_rate\"], momentum=parameters[\"momentum\"])\n",
        "    print(optimizer)\n",
        "\n",
        "    # Early Stopping parameters\n",
        "    patience = 5  # Number of epochs to wait for improvement before stopping\n",
        "    best_val_loss = None  # Best validation loss observed\n",
        "    epochs_no_improve = 0  # Count of epochs with no improvement\n",
        "\n",
        "    train_dataset_size = len(train_dataset)\n",
        "    samples_used_in_epoch = 0\n",
        "    train_losses = []\n",
        "    validation_losses = []\n",
        "    train_accuracies = []\n",
        "    validation_accuracies = []\n",
        "    train_time = []\n",
        "    validation_time = []\n",
        "    best_acc = 0\n",
        "    best_model = None\n",
        "    best_true_labels = None\n",
        "    best_predicted_labels = None\n",
        "    for epoch in range(parameters[\"epochs\"]):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        train_start = time.perf_counter()\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model.forward(inputs)\n",
        "            loss = criterion.forward(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "            samples_used_in_epoch += inputs.size(0)\n",
        "            print(f\"Training {samples_used_in_epoch}/{train_dataset_size}, Loss: {loss.item()}, Running loss: {train_loss} \")\n",
        "        train_end = time.perf_counter()\n",
        "\n",
        "        train_losses.append(train_loss / len(train_loader))\n",
        "        train_accuracy = correct_train / total_train\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        train_time.append(train_end - train_start)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        #Confusion matrix\n",
        "        all_true_labels = []\n",
        "        all_predicted_labels = []\n",
        "        with torch.no_grad():\n",
        "            validation_start = time.perf_counter()\n",
        "            for i, (inputs, labels) in enumerate(test_loader):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model.forward(inputs)\n",
        "                loss = criterion.forward(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                # Collect true and predicted labels\n",
        "                all_true_labels.extend(labels.cpu().numpy())\n",
        "                all_predicted_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "            validation_end = time.perf_counter()\n",
        "\n",
        "        validation_losses.append(val_loss / len(test_loader))\n",
        "        validation_accuracy = correct / total\n",
        "        validation_accuracies.append(validation_accuracy)\n",
        "        validation_time.append(validation_end - validation_start)\n",
        "\n",
        "        if validation_accuracy > best_acc:\n",
        "            best_acc = validation_accuracy\n",
        "            best_model = copy.deepcopy(model)\n",
        "            best_true_labels = all_true_labels\n",
        "            best_predicted_labels = all_predicted_labels\n",
        "\n",
        "        samples_used_in_epoch = 0\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{parameters[\"epochs\"]}, Train Loss: {train_loss}, Avg Train Loss/Batch: {train_loss/len(train_loader)}, Train Acc: {train_accuracy}, Train time: {train_end - train_start}, Val Loss: {val_loss}, Avg Val Loss/Batch: {val_loss/len(test_loader)}, Accuracy: {correct / total}, Val Time: {validation_start - validation_end}')\n",
        "\n",
        "        # Early Stopping\n",
        "        if best_val_loss is None or val_loss < (best_val_loss - 1e-3):\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience and epoch >= 14:\n",
        "                print(\"Early Stopping...\")\n",
        "                break  # Break the training loop\n",
        "\n",
        "    # Plot curves\n",
        "    plot_curves(parameters[\"model\"], train_losses, train_accuracies, validation_losses, validation_accuracies, best_true_labels, best_predicted_labels, suffix)\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(best_model.state_dict(), f'{project_dir}models/fine_tuned_{parameters[\"model\"]}_{suffix}.pth')\n",
        "\n",
        "    results = {\n",
        "        \"train_losses\": train_losses,\n",
        "        \"validation_losses\": validation_losses,\n",
        "        \"train_accuracies\": train_accuracies,\n",
        "        \"validation_accuracies\": validation_accuracies,\n",
        "        \"train_time\": train_time,\n",
        "        \"validation_time\": validation_time,\n",
        "        \"best_acc\": best_acc,\n",
        "    }\n",
        "\n",
        "    with open(f\"{project_dir}plots/{parameters['model']}_{suffix}_data.json\", \"w+\") as file:\n",
        "        json.dump(results, file, indent=4)\n",
        "\n",
        "    return results\n",
        "\n",
        "def plot_curves(model_name, train_loss_history, train_acc_history, valid_loss_history, valid_acc_history, true_labels, predicted_labels, suffix):\n",
        "    # Loss\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(train_loss_history, label='Train', color='blue')\n",
        "    plt.plot(valid_loss_history, label='Validation', color='orange')\n",
        "    plt.title(f'Loss Curve - {model_name}')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='best')\n",
        "    plt.xlim(left=0)\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f'{project_dir}plots/{model_name}_{suffix}_loss_curve.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Accuracy\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(train_acc_history, label='Train', color='blue')\n",
        "    plt.plot(valid_acc_history, label='Validation', color='orange')\n",
        "    plt.title(f'Accuracy Curve - {model_name}')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='best')\n",
        "    plt.xlim(left=0)\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f'{project_dir}plots/{model_name}_{suffix}_accuracy_curve.png')\n",
        "    plt.close()\n",
        "\n",
        "    class_labels = ['0', '1', '2', '3']\n",
        "    # Convert to numpy\n",
        "    cm = confusion_matrix(true_labels, predicted_labels, normalize='true')\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    disp.plot(include_values=True, values_format='.2f')\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.savefig(f'{project_dir}plots/{model_name}_{suffix}_confusion_matrix.png')\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning"
      ],
      "metadata": {
        "id": "Mkmxiec-98de"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKUphhc3-3Kb"
      },
      "outputs": [],
      "source": [
        "parameters = {\n",
        "    # Select model: 'resnet50', 'densenet121', 'mobilenet_v2'\n",
        "    \"model\": \"mobilenet_v2\",\n",
        "    \"train_dir\": f\"{project_dir}filtered/BASE\",\n",
        "    \"test_dir\": f\"{project_dir}test_set\",\n",
        "    \"epochs\": 100,\n",
        "    \"num_workers\": 1,\n",
        "    # Hyperparameters\n",
        "    \"dilation\": False,\n",
        "    \"batch_size\": 64,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"momentum\": 0.99,\n",
        "}\n",
        "\n",
        "\n",
        "batch_sizes = [32, 64, 128]\n",
        "learning_rates = [0.01, 0.001, 0.0001]\n",
        "momentum = [0.9, 0.95, 0.99]\n",
        "\n",
        "accs = []\n",
        "\n",
        "for b in batch_sizes:\n",
        "    for lr in learning_rates:\n",
        "        for m in momentum:\n",
        "            p = parameters.copy()\n",
        "            p[\"batch_size\"] = b\n",
        "            p[\"learning_rate\"] = lr\n",
        "            p[\"momentum\"] = m\n",
        "\n",
        "            suffix = f\"b{str(b)}-lr{str(lr)}-m{str(m)}\"\n",
        "\n",
        "            results = train(p, suffix)\n",
        "            acc = results[\"best_acc\"]\n",
        "            accs.append(f\"{suffix},{str(acc)}\\n\")\n",
        "\n",
        "with open(f\"{project_dir}/plots/results.csv\", \"w+\") as csv:\n",
        "    csv.writelines(accs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run on filtered image sets"
      ],
      "metadata": {
        "id": "W20WhzE4_DAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {\n",
        "    # Select model: 'resnet50', 'densenet121', 'mobilenet_v2'\n",
        "    \"model\": \"mobilenet_v2\",\n",
        "    \"train_dir\": f\"{project_dir}filtered/BASE\",\n",
        "    \"test_dir\": f\"{project_dir}test_set\",\n",
        "    \"epochs\": 100,\n",
        "    \"num_workers\": 1,\n",
        "    # Hyperparameters\n",
        "    \"dilation\": True,\n",
        "    \"batch_size\": 64,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"momentum\": 0.99,\n",
        "}\n",
        "\n",
        "train_types = ['BASE', 'BLACK_WHITE', 'SHARPEN', 'EDGE_ENHANCE', 'DETAIL', 'EMBOSS']\n",
        "accs = []\n",
        "for t in train_types:\n",
        "    p = parameters.copy()\n",
        "    p[\"train_dir\"] = f\"{project_dir}filtered/{t}\"\n",
        "\n",
        "    suffix = t\n",
        "\n",
        "    results = train(p, suffix)\n",
        "    acc = results[\"best_acc\"]\n",
        "    accs.append(f\"{suffix},{str(acc)}\\n\")\n",
        "\n",
        "with open(f\"{project_dir}/plots/results.csv\", \"w+\") as csv:\n",
        "    csv.writelines(accs)"
      ],
      "metadata": {
        "id": "qK7DJYZ5-lfz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}